{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Map Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pds\n",
    "import random\n",
    "import copy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from collections import deque\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "print(K.tensorflow_backend._get_available_gpus())\n",
    "#config = tf.ConfigProto( device_count = {'GPU': 7 , 'CPU': 8} ) \n",
    "#sess = tf.Session(config=config) \n",
    "#keras.backend.set_session(sess)\n",
    "\n",
    "class map(object):\n",
    "    def __init__(self, size=10, blocks_rate=0.1):\n",
    "        self.size = size if size > 3 else 10\n",
    "        self.blocks = int((size ** 2) * blocks_rate) \n",
    "        self.begin_list = []\n",
    "        self.map_list = []\n",
    "        self.e_list = []\n",
    "\n",
    "    def gen_map(self, k):\n",
    "        if k == 0: self.map_list.append(self.begin_list)\n",
    "        elif k == self.size - 1: self.map_list.append(self.e_list)\n",
    "        else:\n",
    "            tmp_list = []\n",
    "            for l in range(0,self.size):\n",
    "                if l == 0: tmp_list.extend(\"#\")\n",
    "                elif l == self.size-1: tmp_list.extend(\"#\")\n",
    "                else:\n",
    "                    a = random.randint(-1, 0)\n",
    "                    tmp_list.extend([a])\n",
    "            self.map_list.append(tmp_list)\n",
    "\n",
    "    def insert_blocks(self, k, begin_random, end_random):\n",
    "        b_y = random.randint(1, self.size-2)\n",
    "        b_x = random.randint(1, self.size-2)\n",
    "        if [b_y, b_x] == [1, begin_random] or [b_y, b_x] == [self.size - 2, end_random]: k = k-1\n",
    "        else: self.map_list[b_y][b_x] = \"#\"\n",
    "            \n",
    "    def generate_map(self): \n",
    "        begin_random = random.randint(1, (self.size / 2) - 1)\n",
    "        for i in range(0, self.size):\n",
    "            if i == begin_random: self.begin_list.extend(\"S\")\n",
    "            else: self.begin_list.extend(\"#\")\n",
    "        start_point = [0, begin_random]\n",
    "\n",
    "        end_random = random.randint((self.size / 2) + 1, self.size - 2)\n",
    "        for j in range(0, self.size):\n",
    "            if j == end_random: self.e_list.extend([100])\n",
    "            else: self.e_list.extend(\"#\")\n",
    "        goal_point = [self.size - 1, end_random]\n",
    "\n",
    "        for k in range(0, self.size):\n",
    "            self.gen_map(k)\n",
    "        \n",
    "        for k in range(self.blocks):\n",
    "            self.insert_blocks(k, begin_random, end_random)\n",
    "\n",
    "        return self.map_list, start_point, goal_point\n",
    "    \n",
    "class Field(object):\n",
    "    def __init__(self, map, start_point, goal_point):\n",
    "        self.map = map\n",
    "        self.start_point = start_point\n",
    "        self.goal_point = goal_point\n",
    "        self.movable_vec = [[1,0],[-1,0],[0,1],[0,-1]]\n",
    "\n",
    "    def display(self, point=None):\n",
    "        field_data = copy.deepcopy(self.map)\n",
    "        if not point is None:\n",
    "                y, x = point\n",
    "                field_data[y][x] = \"{}{}\"\n",
    "        else:\n",
    "                point = \"\"\n",
    "        for line in field_data:\n",
    "                print (\"\\t\" + \"%3s \" * len(line) % tuple(line))\n",
    "\n",
    "    def get_actions(self, state):\n",
    "        movables = []\n",
    "        if state == self.start_point:\n",
    "            y = state[0] + 1\n",
    "            x = state[1]\n",
    "            a = [[y, x]]\n",
    "            return a\n",
    "        else:\n",
    "            for v in self.movable_vec:\n",
    "                y = state[0] + v[0]\n",
    "                x = state[1] + v[1]\n",
    "                if not(0 < x < len(self.map) and\n",
    "                       0 <= y <= len(self.map) - 1 and\n",
    "                       map[y][x] != \"#\" and\n",
    "                       map[y][x] != \"S\"):\n",
    "                    continue\n",
    "                movables.append([y,x])\n",
    "            if len(movables) != 0:\n",
    "                return movables#append a possible state to moveables array if possible to move there\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    def get_val(self, state):\n",
    "        y, x = state\n",
    "        if state == self.start_point: return 0, False\n",
    "        else:\n",
    "            v = float(self.map[y][x])\n",
    "            if state == self.goal_point: \n",
    "                return v, True\n",
    "            else: \n",
    "                return v, False\n",
    "size = 15\n",
    "barrier_rate = 0.1\n",
    "\n",
    "map_1 = map(size, barrier_rate)\n",
    "map, start_point, goal_point = map_1.generate_map()\n",
    "map_field = Field(map, start_point, goal_point)\n",
    "\n",
    "map_field.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Deep Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.gamma = 0.9\n",
    "        self.learning_rate = 1.0\n",
    "        self.e_decay = 0.9999\n",
    "        self.e_min = 0.01\n",
    "        self.learning_rate = 0.0001\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def learning_rate_refresh(self):\n",
    "        self.learning_rate=4*self.learning_rate\n",
    "        if self.learning_rate<0.1:\n",
    "            self.learning_rate=0.25\n",
    "        if self.learning_rate>1.0:\n",
    "            self.learning_rate=1.0\n",
    "        print(\"learning rate refreshed\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_shape=(2,2), activation='tanh'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(128, activation='tanh'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(loss=\"mse\", optimizer=RMSprop(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember_memory(self, state, action, reward, next_state, next_movables, done):\n",
    "        self.memory.append((state, action, reward, next_state, next_movables, done))\n",
    "\n",
    "    def choose_action(self, state, movables):\n",
    "        if self.learning_rate >= random.random():\n",
    "            return random.choice(movables)\n",
    "        else:\n",
    "            return self.choose_best_action(state, movables)\n",
    "        \n",
    "    def choose_best_action(self, state, movables):\n",
    "        best_actions = []\n",
    "        max_act_value = -100\n",
    "        for a in movables:\n",
    "            np_action = np.array([[state, a]])\n",
    "            act_value = self.model.predict(np_action)\n",
    "            if act_value > max_act_value:\n",
    "                best_actions = [a,]\n",
    "                max_act_value = act_value\n",
    "            elif act_value == max_act_value:\n",
    "                best_actions.append(a)\n",
    "        return random.choice(best_actions)\n",
    "\n",
    "    def replay_experience(self, batch_size):\n",
    "        batch_size = min(batch_size, len(self.memory))\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        X = []\n",
    "        Y = []\n",
    "        for i in range(batch_size):\n",
    "            state, action, reward, next_state, next_movables, done = minibatch[i]\n",
    "            input_action = [state, action]\n",
    "            if done:\n",
    "                target_f = reward\n",
    "            else:\n",
    "                next_rewards = []\n",
    "                for i in next_movables:\n",
    "                    np_next_s_a = np.array([[next_state, i]])\n",
    "                    next_rewards.append(self.model.predict(np_next_s_a))\n",
    "                np_n_r_max = np.amax(np.array(next_rewards))\n",
    "                target_f = reward + self.gamma * np_n_r_max\n",
    "            X.append(input_action)\n",
    "            Y.append(target_f)\n",
    "        np_X = np.array(X)\n",
    "        np_Y = np.array([Y]).T\n",
    "        self.model.fit(np_X, np_Y, epochs=1, verbose=0)\n",
    "        if self.learning_rate > self.e_min:\n",
    "            self.learning_rate *= self.e_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refreshing the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracer = DQN(2,2)\n",
    "not_done_count=0\n",
    "done_count=0\n",
    "episodes = 15000\n",
    "steps = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin training of the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = start_point\n",
    "    score = 0\n",
    "    if done_count>500:\n",
    "        break\n",
    "    if e%1000:\n",
    "        done_count=0\n",
    "    for step in range(steps):\n",
    "        movables = map_field.get_actions(state)\n",
    "        action = tracer.choose_action(state, movables)\n",
    "        reward, done = map_field.get_val(action)\n",
    "        score = score + reward\n",
    "        next_state = action\n",
    "        next_movables = map_field.get_actions(next_state)\n",
    "        tracer.remember_memory(state, action, reward, next_state, next_movables, done)\n",
    "        if done:\n",
    "            done_count=done_count+1\n",
    "            not_done_count=0\n",
    "            if e % 100 == 0:\n",
    "                print(\"episode: {}/{}, score: {}, learning rate: {:.2} \\t steps: {}\"\n",
    "                        .format(e, episodes, score, tracer.learning_rate, step))\n",
    "            break\n",
    "        if step == (steps - 1):\n",
    "            done_count=0\n",
    "            not_done_count=not_done_count+1\n",
    "            if e % 100 == 0:\n",
    "                print(\"episode: {}/{}, score: {}, learning rate: {:.2} \\t steps: {}\"\n",
    "                        .format(e, episodes, score, tracer.learning_rate, step))\n",
    "            if not_done_count>1000:\n",
    "                tracer.learning_rate_refresh()\n",
    "                not_done_count=0\n",
    "            break\n",
    "            \n",
    "        prev_state=state\n",
    "        state = next_state\n",
    "    tracer.replay_experience(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The optimised path output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = start_point\n",
    "score = 0\n",
    "steps = 0\n",
    "while True:\n",
    "    steps += 1\n",
    "    movables = map_field.get_actions(state)\n",
    "    action = dql_solver.choose_best_action(state, movables)\n",
    "    print(\"current state: {0} -> action: {1} \".format(state, action))\n",
    "    reward, done = map_field.get_val(action)\n",
    "    map_field.display(state)\n",
    "    score = score + reward\n",
    "    state = action\n",
    "    print(\"current step: {0} \\t score: {1}\\n\".format(steps, score))\n",
    "    if done:\n",
    "        map_field.display(action)\n",
    "        print(\"goal!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
